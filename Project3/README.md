# Emory-2018-ml-project-3 (End April 15, 2018)
A joint project for ML course

# Major Tasks:

## Tune paramters

### batch_size

A too large batch_size will increase the computation load for every backpropogation and a too small batch_size will make the gradients noisy and influence the convergence rate of the optimization algorithm

### conv-net

Try different structures of the conv-net and compare the performance with each other. The components you can modify but not limited to are: 

   - [x] initial learning rate
   
         `for learning_rate in [1E-3, 1E-4, 1E-5]:`
         
         use 1E-1
         
         
   - [x] size of the filters
   
         Filter number:5x5 
   
   - [x] number of filters
   
         first layer 32, second layer 64
   
   - [x] number of conv layers
   
         `for use_two_conv in [True, False]:`
         
         2 
   
   - [x] number of pooling layers
   
          2
   
   
   - [x] the ways of paddings
   
         "same"
   
   - [x] the choices of activation functions
   
   
         RELU
   
   
   Construct a hyperparameter string for each one (example: "lr_1E-3,fc=2,conv=2")
   
      `hparam_str = make_hpram_string(learning_rate, use_two_fc, use_two_conv)
       writer = tf.summary.FileWrite("/tmp/mnist_tutorial/" + hpram_str)`
       
 Then run with the new settings
      `mnist(learnig_rate, use_two_fully_connected_layers, use_two_conv_layers, writer)`
     

### built-in optimizers

[Understand the mathematical mechanisms](https://www.tensorflow.org/api_guides/python/train#Optimizers) and  observe the convergence rate compared to steepest gradient descent.

   - [x] momentum optimizer
   - [x] adam optimizer

## Tensorboard:
 Add tensorboard part to your model. Tensorboard is a powerful tool, which helps you monitor how your training process goes on. It is a good way to visualize your model (computation graph you generated with tensorflow) and help you tune a good combination of hyper-parameters. You can even visualize how the weights of your model change with respect to training epochs. Read the [tutorial](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) and watch the great video on this page for more information on adding tensorboard to your code.

*Hint: In this project, you may not need to initialize `tf.summary.FileWriter` by your own since `tf.estimator.Estimator` will automatically help you write the summary you want to record into the event file (See this discussion). You may just add `tf.summary.scalar('loss', loss)` after the loss computed in the original code and then you can view the result in the tensorboard after the training finishes.

To visualize the tensorboard event file on terminal:

`tensorboard --logdir=path/to/log-directory`.

For me, the default path is `tensorboard --logdir=/tmp/tensorflow/mnist/logs/mnist_with_summaries/test/`

which is a directory contains two subdirectories `train` and `test` which will both be read. 

Then enter `localhost:6006` in a browser to visualize. 



## Write up report

Write up a report about how you did the experiment in step 1) – 4). Report
the performance on testing data based on the best model you have tuned (the structure of the model, the regularization methods you applied to prevent overfitting and any other components you added to make your model better and etc.) and attach the training plots generated by tensorboard to show how the training process goes. You may need to include how training and validation loss changes, how training and validation accuracy changes and the computation graph you generated (in the GRAPH part of the tensorboard log file).



# Group member

* Cai
* Ren
* Wang
